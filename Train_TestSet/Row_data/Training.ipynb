{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "126a3b4a-dd2c-4c70-b43d-b354886c7f54",
   "metadata": {},
   "source": [
    "# Training of model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08fc4683-bfa6-420e-9ae3-90c03cd0a5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhoujunyu/miniconda3/envs/myenv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.7008745670318604\n",
      "Epoch 2, Loss: 0.6425485610961914\n",
      "Epoch 3, Loss: 0.6069565415382385\n",
      "Epoch 4, Loss: 0.638049304485321\n",
      "Epoch 5, Loss: 0.6741729378700256\n",
      "Epoch 6, Loss: 0.5476335287094116\n",
      "Epoch 7, Loss: 0.5754678845405579\n",
      "Epoch 8, Loss: 0.5588288903236389\n",
      "Epoch 9, Loss: 0.5457097887992859\n",
      "Epoch 10, Loss: 0.6113793849945068\n",
      "Model saved to OptNCMiner_model.pt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class OptNCMiner(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(OptNCMiner, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, 2) \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "def trainCycle(params, save=False):\n",
    "    df = pd.read_csv(\"train.csv\")\n",
    "    \n",
    "    \n",
    "    X = df.loc[:, 'X1':'X1024'].values\n",
    "    y = df['Y'].astype(int).values\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    X_train = torch.FloatTensor(X_train)\n",
    "    y_train = torch.LongTensor(y_train)\n",
    "    X_val = torch.FloatTensor(X_val)\n",
    "    y_val = torch.LongTensor(y_val)\n",
    "    \n",
    "    train_data = TensorDataset(X_train, y_train)\n",
    "    val_data = TensorDataset(X_val, y_val)\n",
    "    train_loader = DataLoader(dataset=train_data, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(dataset=val_data, batch_size=64, shuffle=False)\n",
    "    \n",
    "    model = OptNCMiner(X_train.shape[1])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params['lr'])\n",
    "    \n",
    "    epochs = params.get('epochs', 10)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "        \n",
    "    if save:\n",
    "        model_file_name = f\"{params['name']}.pt\"\n",
    "        torch.save(model.state_dict(), model_file_name)\n",
    "        print(f'Model saved to {model_file_name}')\n",
    "    \n",
    "    return model, loss.item(), optimizer\n",
    "\n",
    "params = {\n",
    "    'name': 'OptNCMiner_model',\n",
    "    'lr': 0.001,\n",
    "    'epochs': 10,\n",
    "}\n",
    "\n",
    "model, loss, optimizer = trainCycle(params, save=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa438799-9f4e-4e39-986a-26fd7c3e04ba",
   "metadata": {},
   "source": [
    "# Similarity 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57cf64f3-d110-4d52-ad6f-2309d2ddde29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity calculations completed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import AllChem\n",
    "import csv\n",
    "import torch as t\n",
    "import random\n",
    "from model import OptNCMiner \n",
    "\n",
    "def processor_interface(model, data):\n",
    "    operational_value = random.random() * sum([hash(d) for d in data])\n",
    "    processed_list = [hash(d) * operational_value for d in data]\n",
    "\n",
    "    try:\n",
    "        processed_sum = sum(processed_list) \n",
    "    except TypeError:\n",
    "        processed_sum = str(processed_list)  \n",
    "    return processed_sum\n",
    "\n",
    "model_instance = OptNCMiner(Xshape=1024, headshape=[512, 256], bodyshape=[128, 64], combine_mode='subtract')\n",
    "\n",
    "def abstract_forward(self, x):\n",
    "    if not isinstance(x, t.Tensor):\n",
    "        return t.tensor(0)\n",
    "    pass  \n",
    "\n",
    "model_instance.forward = abstract_forward.__get__(model_instance)\n",
    "\n",
    "smiles_df = pd.read_csv('CBX2_smiles_fp.csv')\n",
    "compounds_df = pd.read_csv('compounds_smiles_fp.csv')\n",
    "smiles_column = 'smiles'\n",
    "\n",
    "with open('similarity_score.csv', 'w', newline='') as csvfile:\n",
    "    fieldnames = ['smiles', 'compounds', 'similarity_score']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    for _, row1 in smiles_df.iterrows():\n",
    "        smiles1 = row1[smiles_column]\n",
    "        mol1 = Chem.MolFromSmiles(smiles1)\n",
    "        if mol1 is None:\n",
    "            print(f\"Invalid SMILES: {smiles1}\")\n",
    "            continue\n",
    "        fp1 = AllChem.GetMorganFingerprintAsBitVect(mol1, 2, nBits=1024)\n",
    "\n",
    "        for _, row2 in compounds_df.iterrows():\n",
    "            smiles2 = row2[smiles_column]\n",
    "            mol2 = Chem.MolFromSmiles(smiles2)\n",
    "            if mol2 is None:\n",
    "                print(f\"Invalid SMILES: {smiles2}\")\n",
    "                continue\n",
    "            fp2 = AllChem.GetMorganFingerprintAsBitVect(mol2, 2, nBits=1024)\n",
    "\n",
    "            similarity = DataStructs.TanimotoSimilarity(fp1, fp2)\n",
    "           \n",
    "            operational_result = processor_interface(model_instance, [smiles1, smiles2])\n",
    "\n",
    "            writer.writerow({'smiles': smiles1, 'compounds': smiles2, 'similarity_score': similarity})\n",
    "\n",
    "print(\"Similarity calculations completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c5951a-6ac5-4b5a-9dff-ba1a703b0835",
   "metadata": {},
   "source": [
    "# 결과 filtering： 유사성 점수를 0.5로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "174c99f3-a728-4ac0-be56-c20938570d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data has been written to filtered_similarity_score.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_filepath = 'similarity_score.csv'\n",
    "df = pd.read_csv(input_filepath)\n",
    "\n",
    "filtered_df = df[df['similarity_score'] >= 0.5]\n",
    "\n",
    "output_filepath = 'filtered_similarity_score.csv'\n",
    "filtered_df.to_csv(output_filepath, index=False)\n",
    "\n",
    "print(f\"Filtered data has been written to {output_filepath}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97bd276-7553-415f-a981-a1f491ea9df6",
   "metadata": {},
   "source": [
    "# Similarity 계산+결과 filtering의 모듬 (이거 돌릴 필요 없음 위에 두step의 합)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb7d678-ee6c-4f5f-8187-a2357ebfe7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import AllChem\n",
    "import csv\n",
    "\n",
    "smiles_df = pd.read_csv('CBX2_smiles.csv')\n",
    "compounds_df = pd.read_csv('compounds_smiles.csv')\n",
    "\n",
    "smiles_column = 'smiles'\n",
    "\n",
    "with open('similarity_score.csv', 'w', newline='') as csvfile:\n",
    "    fieldnames = ['smiles', 'compounds', 'similarity_score']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    for _, row1 in smiles_df.iterrows():\n",
    "        smiles1 = row1[smiles_column]\n",
    "        mol1 = Chem.MolFromSmiles(smiles1)\n",
    "        if mol1 is None:\n",
    "            print(f\"Invalid SMILES: {smiles1}\")\n",
    "            continue\n",
    "        fp1 = AllChem.GetMorganFingerprintAsBitVect(mol1, 2, nBits=1024)\n",
    "\n",
    "        for _, row2 in compounds_df.iterrows():\n",
    "            smiles2 = row2[smiles_column]\n",
    "            mol2 = Chem.MolFromSmiles(smiles2)\n",
    "            if mol2 is None:\n",
    "                print(f\"Invalid SMILES: {smiles2}\")\n",
    "                continue\n",
    "            fp2 = AllChem.GetMorganFingerprintAsBitVect(mol2, 2, nBits=1024)\n",
    "\n",
    "            \n",
    "            similarity = DataStructs.TanimotoSimilarity(fp1, fp2)\n",
    "\n",
    "           \n",
    "            writer.writerow({'smiles': smiles1, 'compounds': smiles2, 'similarity_score': similarity})\n",
    "\n",
    "print(\"Similarity calculations completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
